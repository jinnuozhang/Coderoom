{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层次聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# mpl.rcParams['font.serif'] = ['SimHei']\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"./Font/times.ttf\", size=12)\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "\n",
    "plt.rc('font',family='Times New Roman')\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "plt.rcParams['savefig.dpi'] = 300 #图片像素\n",
    "plt.rcParams['figure.dpi'] = 300 #分辨率\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "#plt.tick_params(labelsize=25)#改坐标轴刻度字体大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要设置的所有参数 #\n",
    "read_root 和 save_root 表示数据读取和存储路径\n",
    "\n",
    "默认层次聚类图存储路径为 read_root/cluster 下 all.png\n",
    "默认单因素聚类图存储路径为 read_root/cluster 下 变量名.png\n",
    "可通过savePicName变量自定义图片名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_root = '.'               #数据存放路径\n",
    "save_root = '.'               #图表存储路径\n",
    "\n",
    "use_all = False               #是否用全部实验数据，如果True则use_d7变量不被考虑\n",
    "if not use_all:\n",
    "    use_d7 =False             #是否仅用第7天, 如果False则用第2天数据\n",
    "use_wave = False               #True表示采用光谱，False表示采用理化值\n",
    "if use_wave:\n",
    "    use_wave_type = 'all'                # 聚类波长的挑选方式： 'all' 全波长; 'select' 特征波长； 'pca' PCA降维\n",
    "    if use_wave_type == 'select':\n",
    "        selected = [23,131,2,64,34,124]  # 如果用特征波长，则给定挑选波长的序号， eg. selected=[23,131,2,64,634,124]\n",
    "    elif use_wave_type == 'pca':\n",
    "        update_pca = False                # 是否更新pca的主成分数\n",
    "        pca_n = 5                         # pca主成分数\n",
    "else:\n",
    "    phenotype = ['M156T536','M171T180','M188T542','M200T536','M232T33','M246T33','M254T59','M281T123','M289T53','M363T35','M405T389',\n",
    "                 'M406T389','M506T56_2','M509T44','M542T535','M63T389','M65T389','M722T38'] #选择的表型参数，根据需要筛减\n",
    "\n",
    "isScale=True                             # 是否归一化\n",
    "measure='distance'                       #’distance‘ 表示采用MMD， ’mean‘表示采用平均值\n",
    "            \n",
    "savePicName_all = '4_24'                     # 设置存储的多因素聚类图名称\n",
    "savePicName_one = ''                     # 设置存储的单因素聚类图名称\n",
    "cmap_color = 'RdGy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 'Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在读取目录下要有这两个文件 biomass.xlsx和wavelength.npy\n",
    "df = pd.read_excel(os.path.join(read_root, 'mmd-me.xlsx'))\n",
    "wavename = np.load(os.path.join(read_root, 'wavelength.npy'))\n",
    "d3 = df[df['time']==3]\n",
    "d7 = df[df['time']==7]\n",
    "if use_all:\n",
    "    dfused = df\n",
    "else:\n",
    "    dfused = d7 if use_d7 else d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorylist = ['M156T536','M171T180','M188T542','M200T536','M232T33','M246T33','M254T59','M281T123','M289T53','M363T35',\n",
    "                'M405T389','M406T389','M506T56_2','M509T44','M542T535','M63T389','M65T389','M722T38']\n",
    "\n",
    "if use_wave:\n",
    "    if use_wave_type == 'all':\n",
    "        attributes = [i for i in range(432)]\n",
    "    elif use_wave_type == 'select':\n",
    "        attributes = selected\n",
    "    else:\n",
    "        attributes = ['p'+str(i) for i in range(5)]\n",
    "else:\n",
    "    attributes = phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(q, p):\n",
    "    len_qz = len(q)\n",
    "    data = np.concatenate([q, p])\n",
    "    for attr in range(data.shape[1]):\n",
    "        Max = np.max(data[:, attr])\n",
    "        Min = np.min(data[:, attr])\n",
    "        data[:, attr] = (Max - data[:, attr]) / (Max - Min)\n",
    "    return data[:len_qz], data[len_qz:]\n",
    "\n",
    "\n",
    "def mmd_penalty_with_p(sample_qz, sample_pz, kernel='RBF', with_scale=True):\n",
    "    '''\n",
    "    p| * q_\n",
    "    :param sample_qz:\n",
    "    :param sample_pz:\n",
    "    :param kernel:\n",
    "    :return:\n",
    "    '''\n",
    "    if len(sample_qz.shape)==1:\n",
    "        sample_qz = sample_qz[:, np.newaxis]\n",
    "        sample_pz = sample_pz[:, np.newaxis]\n",
    "        if with_scale:\n",
    "            sample_qz, sample_pz = Scale(sample_qz, sample_pz)\n",
    "\n",
    "\n",
    "    cross_pz = np.matmul(sample_pz, sample_pz.T)\n",
    "    distances_pz = cross_pz.diagonal()[None, :] + cross_pz.diagonal()[:, None] - 2 * cross_pz\n",
    "\n",
    "    cross_qz = np.matmul(sample_qz, sample_qz.T)\n",
    "    distances_qz = cross_qz.diagonal()[None, :] + cross_qz.diagonal()[:, None] - 2 * cross_qz\n",
    "\n",
    "    distances = cross_pz.diagonal()[:, None] + cross_qz.diagonal()[None, :] - 2 * np.matmul(sample_pz, sample_qz.T)\n",
    "\n",
    "    # sample_pz = (sample_pz-Min)/(Max-Min)\n",
    "    m = len(sample_pz)\n",
    "    n = len(sample_qz)\n",
    "    #   print('n', n)\n",
    "    q_ = (np.ones(n)/n)[:, np.newaxis]\n",
    "    p_ = (np.ones(m)/m)[:, np.newaxis]\n",
    "\n",
    "    if kernel=='IMQ':\n",
    "        # sigma2_p = 1\n",
    "        # # print(q_.shape)\n",
    "        # if config.mmd_pz == 'normal':\n",
    "        #     Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "        # elif config.mmd_pz == 'sphere':\n",
    "        #     Cbase = 2.\n",
    "        # elif config.mmd_pz == 'uniform':\n",
    "        #     Cbase = opts['zdim']\n",
    "        # stat = 0.\n",
    "        # for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "        #     C = Cbase * scale\n",
    "        #     res1 = torch.sum( np.matmul(q_, q_.T) * C / (C + distances_qz))  # / (n**2 - n))\n",
    "        #     res1 += torch.sum( np.matmul(p_, p_.T) * C / (C + distances_pz))  # / (m**2 - m))\n",
    "        #     res2 = torch.sum( np.matmul(p_, q_.T) * C / (C + distances) * 2.)\n",
    "        #     # res2 = torch.sum(res2) * 2. / (n * m)\n",
    "        #     stat += res1 - res2\n",
    "        pass\n",
    "    elif kernel == 'RBF':\n",
    "        sigma2_k = 8\n",
    "        res1 = np.sum( np.exp(distances_qz / -2./sigma2_k) * np.matmul(q_, q_.T))\n",
    "        res1 +=np.sum( np.exp(distances_pz / -2./sigma2_k) * np.matmul(p_, p_.T))\n",
    "        res2 = np.sum( np.exp(distances / -2./ sigma2_k) * np.matmul(p_, q_.T)) * 2\n",
    "        stat = res1 - res2\n",
    "    return stat\n",
    "\n",
    "def convert(df):\n",
    "    attributes = ['M156T536','M171T180','M188T542','M200T536','M232T33','M246T33','M254T59','M281T123','M289T53','M363T35','M405T389','M406T389','M506T56_2','M509T44','M542T535','M63T389','M65T389','M722T38' \n",
    "                   ]\n",
    "    df_ = pd.DataFrame()\n",
    "    df_['category']=None\n",
    "    df_['attr'] = None\n",
    "    df_['value'] = None\n",
    "    h = 0\n",
    "    for i in range(len(df)):\n",
    "        for a in attributes:\n",
    "            df_.loc[str(h)] =[df.loc[i].category, a, df.loc[i][a]]\n",
    "            h += 1\n",
    "    return df_\n",
    "\n",
    "def drawClusterMap(df, title, isSave=True):\n",
    "    sns.set()\n",
    "    if True:\n",
    "        flights = df.set_index('category')\n",
    "    else:\n",
    "        flights_long = convert(df)\n",
    "        flights = flights_long.pivot(\"category\", \"attr\", \"value\")\n",
    "    g = sns.clustermap(flights, fmt=\"d\", cmap=\"RdBu\")\n",
    "    ax = g.ax_heatmap\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.title(title,fontproperties=font)\n",
    "    plt.setp(label_y, rotation=360, horizontalalignment='left',fontproperties=font)\n",
    "    if isSave:\n",
    "        if savePicName_all=='':\n",
    "            plt.savefig(os.path.join(save_root, 'cluster/all.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_root, 'cluster/{}.png'.format(savePicName_all)))\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def drawg(df, attr):\n",
    "    mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    if isinstance(attr, int):\n",
    "        attr = str(wavename[attr])\n",
    "    df = df[['category',attr]]\n",
    "    df = df.set_index('category')\n",
    "    Z = hierarchy.linkage(df, method='ward', metric='euclidean')\n",
    "    hierarchy.dendrogram(Z, labels=df.index,leaf_font_size=45,leaf_rotation=45,distance_sort='ascending')\n",
    "    if savePicName_one=='':\n",
    "        plt.savefig(os.path.join(save_root, 'cluster/{}.png'.format(attr)))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(save_root, 'cluster/{}.png'.format(savePicName_one)))\n",
    "def CalDiff(data, caltype='distance', with_scale=False,SaveDiff=False, SavePic=True):\n",
    "    '''\n",
    "    caltype: 'distance' or 'mean'\n",
    "    '''\n",
    "    result = {'category':[]}\n",
    "    for attr in attributes:\n",
    "        if use_wave and use_wave_type!='pca':\n",
    "            result.update({str(wavename[attr]):[]})\n",
    "        else:\n",
    "            result.update({attr:[]})\n",
    "    for cate in categorylist:\n",
    "        data_cate = data[data['category']==cate]\n",
    "        data_cate_ck = data_cate[data_cate['deal'].str.contains('ck')]\n",
    "        data_cate_nl = data_cate[data_cate['deal'].str.contains('nacl')]\n",
    "        result['category'].append(cate)\n",
    "\n",
    "        for attr in attributes:\n",
    "            if use_wave and use_wave_type!='pca':\n",
    "                str_attr = str(wavename[attr])\n",
    "                attr = 'v'+str(attr)\n",
    "            else:\n",
    "                str_attr = attr\n",
    "                \n",
    "#             if str_attr=='vis':\n",
    "#                 attr = data.columns[19:19+432]\n",
    "#             elif str_attr=='nir':\n",
    "#                 attr = data.columns[451:451+200]\n",
    "                \n",
    "            data_cate_ck_attr = data_cate_ck[attr].values\n",
    "            data_cate_nl_attr = data_cate_nl[attr].values\n",
    "            data_cate_ck_attr = data_cate_ck_attr[~np.isnan(data_cate_ck_attr)]\n",
    "            data_cate_nl_attr = data_cate_nl_attr[~np.isnan(data_cate_nl_attr)]\n",
    "            if caltype=='distance':\n",
    "                dist = mmd_penalty_with_p(data_cate_ck_attr, data_cate_nl_attr, with_scale=with_scale)\n",
    "            else:\n",
    "                dist = np.mean(data_cate_ck_attr)\n",
    "            result[str_attr].append(dist)\n",
    "    df = pd.DataFrame(result)\n",
    "    if SaveDiff:\n",
    "        df.to_excel(os.path.join(save_root, 'dist.xlsx'))\n",
    "    drawClusterMap(df, title=caltype, isSave=SavePic)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-962697ad038a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCalDiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaltype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misScale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSaveDiff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSavePic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-2420aed5af41>\u001b[0m in \u001b[0;36mCalDiff\u001b[1;34m(data, caltype, with_scale, SaveDiff, SavePic)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mdata_cate_nl_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_cate_nl_attr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cate_nl_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcaltype\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'distance'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                 \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmd_penalty_with_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cate_ck_attr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_cate_nl_attr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cate_ck_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2420aed5af41>\u001b[0m in \u001b[0;36mmmd_penalty_with_p\u001b[1;34m(sample_qz, sample_pz, kernel, with_scale)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0msample_pz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_pz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwith_scale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0msample_qz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_pz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_qz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_pz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2420aed5af41>\u001b[0m in \u001b[0;36mScale\u001b[1;34m(q, p)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mMax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mMin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMax\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMax\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user0\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[1;32m-> 2320\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user0\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# small reductions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "result = CalDiff(dfused, caltype=measure, with_scale=isScale,SaveDiff=True, SavePic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单因素层次聚类 #\n",
    "选择attributes中的某个变量名进行聚类，需要先运行上一句得到df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawg的第二个变量为要聚类的变量名，必须在attributes中已有;若是光谱则输入序号\n",
    "drawg(result, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更新pca列，大概不需要 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_pca:\n",
    "    from sklearn.decomposition import PCA  \n",
    "    assert(pca_n>0, '主成分数需要大于0')\n",
    "    pca = PCA(n_components=pca_n)\n",
    "    reduced_N=pca.fit_transform(N)\n",
    "    for i in range(5):\n",
    "        df['p'+str(i)] = reduced_N[:,i]\n",
    "    df.to_excel(os.path.join(read_root, '/biomass.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
